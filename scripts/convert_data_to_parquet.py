#!/usr/bin/env python3
"""
CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’Parquetå½¢å¼ã«å¤‰æ›ã—ã¦ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚’é«˜é€ŸåŒ–ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä¸»ãªæ©Ÿèƒ½:
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒãƒ£ãƒ³ã‚¯å‡¦ç†
- ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–ï¼ˆfloat64â†’float32, int64â†’int32ï¼‰
- å¤‰æ›å‰å¾Œã®ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
- èª­ã¿è¾¼ã¿é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
"""

import time
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq


def optimize_dtypes(df: pd.DataFrame) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿å‹ã‚’æœ€é©åŒ–ã—ã¦ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›"""
    print("ãƒ‡ãƒ¼ã‚¿å‹ã‚’æœ€é©åŒ–ä¸­...")
    original_memory = df.memory_usage(deep=True).sum() / 1024**2

    for col in df.columns:
        col_type = df[col].dtype

        if col_type != "object":
            # æ•°å€¤å‹ã®æœ€é©åŒ–
            if "int" in str(col_type):
                # æ•´æ•°å‹ã®æœ€é©åŒ–
                if (
                    df[col].min() >= np.iinfo(np.int32).min
                    and df[col].max() <= np.iinfo(np.int32).max
                ):
                    df[col] = df[col].astype(np.int32)
                elif (
                    df[col].min() >= np.iinfo(np.int16).min
                    and df[col].max() <= np.iinfo(np.int16).max
                ):
                    df[col] = df[col].astype(np.int16)
            elif "float" in str(col_type):
                # æµ®å‹•å°æ•°ç‚¹å‹ã®æœ€é©åŒ–
                df[col] = pd.to_numeric(df[col], downcast="float")

    optimized_memory = df.memory_usage(deep=True).sum() / 1024**2
    print(
        f"ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {original_memory:.2f}MB â†’ {optimized_memory:.2f}MB "
        f"({((original_memory - optimized_memory) / original_memory * 100):.1f}% å‰Šæ¸›)"
    )

    return df


def convert_csv_to_parquet(
    csv_path: Path, parquet_path: Path, chunksize: int = 100000
) -> dict[str, Any]:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’Parquetå½¢å¼ã«å¤‰æ›"""
    print(f"\n=== {csv_path.name} â†’ {parquet_path.name} ===")

    start_time = time.time()
    csv_size = csv_path.stat().st_size / 1024**2  # MB

    # ãƒãƒ£ãƒ³ã‚¯å˜ä½ã§CSVã‚’èª­ã¿è¾¼ã¿ã€Parquetã«æ›¸ãè¾¼ã¿
    print(f"CSVèª­ã¿è¾¼ã¿é–‹å§‹ (ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {csv_size:.1f}MB)")

    first_chunk = True
    total_rows = 0
    parquet_writer = None

    try:
        for chunk_idx, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunksize)):
            print(f"ãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1} å‡¦ç†ä¸­... (è¡Œæ•°: {len(chunk)})")

            # ãƒ‡ãƒ¼ã‚¿å‹æœ€é©åŒ–
            chunk = optimize_dtypes(chunk)
            total_rows += len(chunk)

            # Parquetãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
            table = pa.Table.from_pandas(chunk)

            if first_chunk:
                parquet_writer = pq.ParquetWriter(parquet_path, table.schema)
                first_chunk = False

            parquet_writer.write_table(table)

    finally:
        if parquet_writer:
            parquet_writer.close()

    conversion_time = time.time() - start_time
    parquet_size = parquet_path.stat().st_size / 1024**2  # MB

    print("å¤‰æ›å®Œäº†!")
    print(f"  å‡¦ç†æ™‚é–“: {conversion_time:.1f}ç§’")
    print(f"  ç·è¡Œæ•°: {total_rows:,}")
    print(
        f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {csv_size:.1f}MB â†’ {parquet_size:.1f}MB "
        f"({((csv_size - parquet_size) / csv_size * 100):.1f}% å‰Šæ¸›)"
    )

    return {
        "original_size_mb": csv_size,
        "converted_size_mb": parquet_size,
        "conversion_time_sec": conversion_time,
        "total_rows": total_rows,
        "compression_ratio": csv_size / parquet_size,
    }


def verify_data_integrity(
    csv_path: Path, parquet_path: Path, sample_size: int = 10000
) -> bool:
    """ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    print("\n=== ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ ===")

    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã§æ¯”è¼ƒ
    print("CSVã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    csv_sample = pd.read_csv(csv_path, nrows=sample_size)

    print("Parquetã‚µãƒ³ãƒ—ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    parquet_sample = pd.read_parquet(parquet_path).head(sample_size)

    # åŸºæœ¬ãƒã‚§ãƒƒã‚¯
    print("åŸºæœ¬æƒ…å ±æ¯”è¼ƒ:")
    print(f"  è¡Œæ•°: CSV={len(csv_sample)}, Parquet={len(parquet_sample)}")
    print(
        f"  åˆ—æ•°: CSV={len(csv_sample.columns)}, Parquet={len(parquet_sample.columns)}"
    )

    # åˆ—åãƒã‚§ãƒƒã‚¯
    if not csv_sample.columns.equals(parquet_sample.columns):
        print("âŒ åˆ—åãŒä¸€è‡´ã—ã¾ã›ã‚“")
        return False

    # æ•°å€¤ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ï¼ˆå‹å¤‰æ›ã«ã‚ˆã‚‹ç²¾åº¦æå¤±ã‚’è€ƒæ…®ï¼‰
    numeric_cols = csv_sample.select_dtypes(include=[np.number]).columns
    for col in numeric_cols[:5]:  # æœ€åˆã®5åˆ—ã‚’ãƒã‚§ãƒƒã‚¯
        csv_values = csv_sample[col].fillna(0)
        parquet_values = parquet_sample[col].fillna(0)

        # ç›¸å¯¾èª¤å·®ãƒã‚§ãƒƒã‚¯ï¼ˆå‹å¤‰æ›ã«ã‚ˆã‚‹å°ã•ãªèª¤å·®ã¯è¨±å®¹ï¼‰
        max_error = np.max(
            np.abs(csv_values - parquet_values) / (np.abs(csv_values) + 1e-10)
        )
        if max_error > 1e-6:
            print(f"âŒ åˆ— '{col}' ã§å¤§ããªèª¤å·®ã‚’æ¤œå‡º: {max_error}")
            return False

    print("âœ… ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯å®Œäº†")
    return True


def benchmark_loading_speed(
    csv_path: Path, parquet_path: Path, iterations: int = 3
) -> dict[str, float]:
    """èª­ã¿è¾¼ã¿é€Ÿåº¦ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    print("\n=== èª­ã¿è¾¼ã¿é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ ===")

    csv_times = []
    parquet_times = []

    print("CSVèª­ã¿è¾¼ã¿é€Ÿåº¦æ¸¬å®š...")
    for i in range(iterations):
        start_time = time.time()
        _ = pd.read_csv(csv_path)
        csv_time = time.time() - start_time
        csv_times.append(csv_time)
        print(f"  è©¦è¡Œ {i + 1}: {csv_time:.2f}ç§’")

    print("Parquetèª­ã¿è¾¼ã¿é€Ÿåº¦æ¸¬å®š...")
    for i in range(iterations):
        start_time = time.time()
        _ = pd.read_parquet(parquet_path)
        parquet_time = time.time() - start_time
        parquet_times.append(parquet_time)
        print(f"  è©¦è¡Œ {i + 1}: {parquet_time:.2f}ç§’")

    avg_csv_time = np.mean(csv_times)
    avg_parquet_time = np.mean(parquet_times)
    speedup = avg_csv_time / avg_parquet_time

    print("\nçµæœ:")
    print(f"  CSVå¹³å‡èª­ã¿è¾¼ã¿æ™‚é–“: {avg_csv_time:.2f}ç§’")
    print(f"  Parquetå¹³å‡èª­ã¿è¾¼ã¿æ™‚é–“: {avg_parquet_time:.2f}ç§’")
    print(f"  é€Ÿåº¦å‘ä¸Š: {speedup:.1f}å€")

    return {
        "csv_avg_time": avg_csv_time,
        "parquet_avg_time": avg_parquet_time,
        "speedup_factor": speedup,
    }


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    data_dir = Path("data")

    # å¤‰æ›å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
    files_to_convert = [
        ("train.csv", "train.parquet"),
        ("test.csv", "test.parquet"),
    ]

    results = {}

    print("ğŸš€ CSV â†’ Parquetå¤‰æ›é–‹å§‹")
    print("=" * 50)

    for csv_name, parquet_name in files_to_convert:
        csv_path = data_dir / csv_name
        parquet_path = data_dir / parquet_name

        if not csv_path.exists():
            print(f"âš ï¸  {csv_name} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        if parquet_path.exists():
            print(f"âš ï¸  {parquet_name} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚ä¸Šæ›¸ãã—ã¾ã™ã€‚")

        # å¤‰æ›å®Ÿè¡Œ
        conversion_result = convert_csv_to_parquet(csv_path, parquet_path)

        # ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        integrity_ok = verify_data_integrity(csv_path, parquet_path)

        # èª­ã¿è¾¼ã¿é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆå°ã•ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ï¼‰
        if csv_path.stat().st_size < 500 * 1024 * 1024:  # 500MBæœªæº€
            benchmark_result = benchmark_loading_speed(csv_path, parquet_path)
            conversion_result.update(benchmark_result)

        conversion_result["integrity_check"] = integrity_ok
        results[csv_name] = conversion_result

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "=" * 50)
    print("ğŸ‰ å¤‰æ›å®Œäº†ã‚µãƒãƒªãƒ¼")
    print("=" * 50)

    total_original_size = 0
    total_converted_size = 0

    for filename, result in results.items():
        print(f"\nğŸ“„ {filename}:")
        print(
            f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›: {result['original_size_mb']:.1f}MB â†’ "
            f"{result['converted_size_mb']:.1f}MB"
        )
        print(f"  åœ§ç¸®ç‡: {result['compression_ratio']:.1f}å€")
        print(f"  å¤‰æ›æ™‚é–“: {result['conversion_time_sec']:.1f}ç§’")
        print(f"  ãƒ‡ãƒ¼ã‚¿æ•´åˆæ€§: {'âœ… OK' if result['integrity_check'] else 'âŒ NG'}")

        if "speedup_factor" in result:
            print(f"  èª­ã¿è¾¼ã¿é«˜é€ŸåŒ–: {result['speedup_factor']:.1f}å€")

        total_original_size += result["original_size_mb"]
        total_converted_size += result["converted_size_mb"]

    print("\nğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼:")
    print(
        f"  ç·ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {total_original_size:.1f}MB â†’ "
        f"{total_converted_size:.1f}MB"
    )
    print(f"  ç·åœ§ç¸®ç‡: {total_original_size / total_converted_size:.1f}å€")
    print(f"  ç¯€ç´„å®¹é‡: {total_original_size - total_converted_size:.1f}MB")

    print("\nâœ¨ å¤‰æ›å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
