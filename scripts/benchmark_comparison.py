#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é€Ÿåº¦ã¨ç²¾åº¦å‘ä¸Šã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¯”è¼ƒ
"""

import json
import time
from pathlib import Path

import matplotlib.pyplot as plt
import pandas as pd


def benchmark_loading_speed():
    """CSV vs Parquetèª­ã¿è¾¼ã¿é€Ÿåº¦æ¯”è¼ƒ"""
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 40)

    results = {}

    # CSVèª­ã¿è¾¼ã¿
    print("CSVèª­ã¿è¾¼ã¿æ¸¬å®šä¸­...")
    start_time = time.time()
    _ = pd.read_csv("data/train.csv")
    csv_time = time.time() - start_time
    csv_size = Path("data/train.csv").stat().st_size / 1024**2  # MB

    # Parquetèª­ã¿è¾¼ã¿
    print("Parquetèª­ã¿è¾¼ã¿æ¸¬å®šä¸­...")
    start_time = time.time()
    _ = pd.read_parquet("data/train.parquet")
    parquet_time = time.time() - start_time
    parquet_size = Path("data/train.parquet").stat().st_size / 1024**2  # MB

    results = {
        "csv_load_time": csv_time,
        "parquet_load_time": parquet_time,
        "csv_size_mb": csv_size,
        "parquet_size_mb": parquet_size,
        "speedup_factor": csv_time / parquet_time,
        "compression_ratio": csv_size / parquet_size,
    }

    print(f"CSVèª­ã¿è¾¼ã¿æ™‚é–“: {csv_time:.2f}ç§’")
    print(f"Parquetèª­ã¿è¾¼ã¿æ™‚é–“: {parquet_time:.2f}ç§’")
    print(f"é€Ÿåº¦å‘ä¸Š: {results['speedup_factor']:.1f}å€")
    print(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {csv_size:.1f}MB â†’ {parquet_size:.1f}MB")
    print(f"åœ§ç¸®ç‡: {results['compression_ratio']:.1f}å€")

    return results


def create_comparison_chart():
    """æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆã‚’ä½œæˆ"""
    # å®Ÿé¨“çµæœãƒ‡ãƒ¼ã‚¿
    experiments = {
        "Quick Baseline\n(50k samples)": {
            "accuracy": 0.7361,
            "samples": 50000,
            "data_format": "CSV",
        },
        "Full Dataset\n(575k samples)": {
            "accuracy": 0.8393,
            "samples": 574945,
            "data_format": "Parquet",
        },
    }

    # ãƒãƒ£ãƒ¼ãƒˆä½œæˆ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # ç²¾åº¦æ¯”è¼ƒ
    names = list(experiments.keys())
    accuracies = [exp["accuracy"] for exp in experiments.values()]
    colors = ["lightblue", "lightgreen"]

    bars1 = ax1.bar(names, accuracies, color=colors)
    ax1.set_ylabel("æ¤œè¨¼ç²¾åº¦")
    ax1.set_title("ç²¾åº¦æ¯”è¼ƒ")
    ax1.set_ylim(0.7, 0.85)

    # å€¤ã‚’è¡¨ç¤º
    for bar, acc in zip(bars1, accuracies, strict=False):
        ax1.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.002,
            f"{acc:.3f}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # ã‚µãƒ³ãƒ—ãƒ«æ•°æ¯”è¼ƒ
    samples = [exp["samples"] for exp in experiments.values()]
    bars2 = ax2.bar(names, samples, color=colors)
    ax2.set_ylabel("ã‚µãƒ³ãƒ—ãƒ«æ•°")
    ax2.set_title("ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿é‡æ¯”è¼ƒ")
    ax2.ticklabel_format(style="scientific", axis="y", scilimits=(0, 0))

    # å€¤ã‚’è¡¨ç¤º
    for bar, sample in zip(bars2, samples, strict=False):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + max(samples) * 0.01,
            f"{sample:,}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    plt.tight_layout()
    plt.savefig("results/experiment_comparison.png", dpi=300, bbox_inches="tight")
    plt.show()

    return experiments


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæœ€é©åŒ–å®Ÿé¨“ - ç·åˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)

    # èª­ã¿è¾¼ã¿é€Ÿåº¦ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    loading_benchmark = benchmark_loading_speed()

    # æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆä½œæˆ
    print("\nğŸ“ˆ æ¯”è¼ƒãƒãƒ£ãƒ¼ãƒˆä½œæˆä¸­...")
    experiment_data = create_comparison_chart()

    # ç·åˆãƒ¬ãƒãƒ¼ãƒˆ
    print("\n" + "=" * 60)
    print("ğŸ‰ å®Ÿé¨“ç·åˆãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 60)

    print("ğŸ“„ ãƒ‡ãƒ¼ã‚¿æœ€é©åŒ–åŠ¹æœ:")
    print(f"  ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›: {loading_benchmark['compression_ratio']:.1f}å€")
    print(f"  èª­ã¿è¾¼ã¿é€Ÿåº¦å‘ä¸Š: {loading_benchmark['speedup_factor']:.1f}å€")

    print("\nğŸ¯ ç²¾åº¦æ”¹å–„åŠ¹æœ:")
    baseline_acc = 0.7361
    full_acc = 0.8393
    improvement = full_acc - baseline_acc
    print(f"  ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç²¾åº¦: {baseline_acc:.4f}")
    print(f"  å…¨ãƒ‡ãƒ¼ã‚¿ç²¾åº¦: {full_acc:.4f}")
    print(f"  ç²¾åº¦å‘ä¸Š: +{improvement:.4f} ({improvement / baseline_acc * 100:+.1f}%)")

    print("\nğŸ“Š å®Ÿé¨“ã‚¹ã‚±ãƒ¼ãƒ«:")
    print(f"  ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿é‡: 50,000 â†’ 574,945 ã‚µãƒ³ãƒ—ãƒ« ({574945 / 50000:.1f}å€)")
    print(f"  ç‰¹å¾´é‡æ•°: 50 â†’ 333 ({333 / 50:.1f}å€)")

    # ç·åˆçµæœã‚’ä¿å­˜
    comprehensive_results = {
        "experiment_name": "full_dataset_optimization_benchmark",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "data_optimization": loading_benchmark,
        "accuracy_results": experiment_data,
        "summary": {
            "baseline_accuracy": baseline_acc,
            "optimized_accuracy": full_acc,
            "accuracy_improvement": improvement,
            "improvement_percentage": improvement / baseline_acc * 100,
            "data_scale_increase": 574945 / 50000,
            "feature_scale_increase": 333 / 50,
        },
    }

    # çµæœä¿å­˜
    output_dir = Path("results")
    output_dir.mkdir(exist_ok=True)

    with open(output_dir / "comprehensive_benchmark.json", "w") as f:
        json.dump(comprehensive_results, f, indent=2)

    print(f"\nğŸ’¾ ç·åˆçµæœä¿å­˜: {output_dir / 'comprehensive_benchmark.json'}")
    print("âœ¨ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼")


if __name__ == "__main__":
    main()
