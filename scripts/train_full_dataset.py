#!/usr/bin/env python3
"""
å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ´»ç”¨ã—ãŸæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ä¸»ãªæ©Ÿèƒ½:
- Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®é«˜é€Ÿãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å‡¦ç†
- è¤‡æ•°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã§ã®å­¦ç¿’ã¨æ¯”è¼ƒ
- å±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã‚ˆã‚‹é©åˆ‡ãªæ¤œè¨¼ã‚»ãƒƒãƒˆåˆ†å‰²
- è©³ç´°ãªæ€§èƒ½è©•ä¾¡ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import json
import sys
import time
import warnings
from pathlib import Path
from typing import Any

import lightgbm as lgb
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.model_selection import train_test_split

warnings.filterwarnings("ignore")

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append("src")


def load_data(use_parquet: bool = True) -> tuple[pd.DataFrame, pd.DataFrame]:
    """ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
    data_dir = Path("data")

    if use_parquet:
        train_path = data_dir / "train.parquet"
        test_path = data_dir / "test.parquet"

        if not train_path.exists():
            raise FileNotFoundError(f"Parquetãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {train_path}")

        print("Parquetãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...")
        start_time = time.time()
        train_df = pd.read_parquet(train_path)
        test_df = pd.read_parquet(test_path)
        load_time = time.time() - start_time
        print(f"èª­ã¿è¾¼ã¿å®Œäº†: {load_time:.2f}ç§’")

    else:
        print("CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ä¸­...")
        start_time = time.time()
        train_df = pd.read_csv(data_dir / "train.csv")
        test_df = pd.read_csv(data_dir / "test.csv")
        load_time = time.time() - start_time
        print(f"èª­ã¿è¾¼ã¿å®Œäº†: {load_time:.2f}ç§’")

    print(f"è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_df.shape}")
    print(f"ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape}")

    return train_df, test_df


def prepare_features(
    train_df: pd.DataFrame, test_df: pd.DataFrame
) -> tuple[np.ndarray, np.ndarray, np.ndarray, list[str]]:
    """ç‰¹å¾´é‡ã‚’æº–å‚™"""
    print("\n=== ç‰¹å¾´é‡æº–å‚™ ===")

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’ç‰¹å®š
    target_col = None
    for col in ["behavior", "target", "label"]:
        if col in train_df.columns:
            target_col = col
            break

    if target_col is None:
        raise ValueError("ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—: {target_col}")

    # æ•°å€¤ç‰¹å¾´é‡ã‚’é¸æŠ
    feature_cols = train_df.select_dtypes(include=[np.number]).columns.tolist()
    if target_col in feature_cols:
        feature_cols.remove(target_col)

    # IDåˆ—ã‚’é™¤å¤–
    id_cols = [col for col in feature_cols if "id" in col.lower()]
    for col in id_cols:
        if col in feature_cols:
            feature_cols.remove(col)

    print(f"ä½¿ç”¨ç‰¹å¾´é‡æ•°: {len(feature_cols)}")
    print(f"é™¤å¤–ã•ã‚ŒãŸIDåˆ—: {id_cols}")

    # ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åˆ†é›¢
    X = train_df[feature_cols].values
    y = train_df[target_col].values
    X_test = test_df[feature_cols].values

    # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®å‹ã‚’ãƒã‚§ãƒƒã‚¯ãƒ»å¤‰æ›
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®å‹: {y.dtype}")
    print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯å€¤: {np.unique(y)}")

    # æ–‡å­—åˆ—ã®å ´åˆã¯æ•°å€¤ã«å¤‰æ›
    if y.dtype == "O":  # objectå‹ï¼ˆæ–‡å­—åˆ—ï¼‰ã®å ´åˆ
        from sklearn.preprocessing import LabelEncoder

        le = LabelEncoder()
        y = le.fit_transform(y)
        print(f"ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å¾Œ: {np.unique(y)}")
        class_mapping = dict(zip(le.classes_, le.transform(le.classes_), strict=False))
        print(f"ã‚¯ãƒ©ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°: {class_mapping}")

    # æ¬ æå€¤ãƒã‚§ãƒƒã‚¯
    missing_count = np.isnan(X).sum()
    if missing_count > 0:
        print(f"âš ï¸  æ¬ æå€¤ã‚’æ¤œå‡º: {missing_count}å€‹")
        print("æ¬ æå€¤ã‚’0ã§åŸ‹ã‚ã¾ã™")
        X = np.nan_to_num(X, nan=0.0)
        X_test = np.nan_to_num(X_test, nan=0.0)

    print(f"ç‰¹å¾´é‡è¡Œåˆ—: {X.shape}")
    print(f"ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {y.shape}")
    print(f"ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.bincount(y)}")

    return X, y, X_test, feature_cols


def train_random_forest(
    X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray
) -> tuple[RandomForestClassifier, dict[str, Any]]:
    """RandomForestãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
    print("\n=== RandomForestè¨“ç·´ ===")

    start_time = time.time()

    # ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—ï¼ˆä¸å‡è¡¡å¯¾ç­–ï¼‰
    class_weights = "balanced"

    model = RandomForestClassifier(
        n_estimators=200,
        max_depth=20,
        min_samples_split=5,
        min_samples_leaf=2,
        class_weight=class_weights,
        random_state=42,
        n_jobs=-1,
        verbose=1,
    )

    print(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹... (ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_train):,})")
    model.fit(X_train, y_train)
    train_time = time.time() - start_time

    # äºˆæ¸¬ã¨è©•ä¾¡
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    print(f"è¨“ç·´æ™‚é–“: {train_time:.1f}ç§’")
    print(f"æ¤œè¨¼ç²¾åº¦: {accuracy:.4f}")

    return model, {
        "algorithm": "RandomForest",
        "accuracy": accuracy,
        "train_time": train_time,
        "n_estimators": 200,
        "max_depth": 20,
    }


def train_lightgbm(
    X_train: np.ndarray, y_train: np.ndarray, X_val: np.ndarray, y_val: np.ndarray
) -> tuple[lgb.LGBMClassifier, dict[str, Any]]:
    """LightGBMãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´"""
    print("\n=== LightGBMè¨“ç·´ ===")

    start_time = time.time()

    # ã‚¯ãƒ©ã‚¹é‡ã¿ã‚’è¨ˆç®—
    class_weights = "balanced"

    model = lgb.LGBMClassifier(
        objective="multiclass",
        num_class=len(np.unique(y_train)),
        boosting_type="gbdt",
        num_leaves=31,
        learning_rate=0.05,
        feature_fraction=0.9,
        bagging_fraction=0.8,
        bagging_freq=5,
        verbose=0,
        class_weight=class_weights,
        random_state=42,
        n_estimators=500,
    )

    print(f"ãƒ¢ãƒ‡ãƒ«è¨“ç·´é–‹å§‹... (ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_train):,})")
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_val, y_val)],
        eval_metric="multi_logloss",
        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(0)],
    )
    train_time = time.time() - start_time

    # äºˆæ¸¬ã¨è©•ä¾¡
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    print(f"è¨“ç·´æ™‚é–“: {train_time:.1f}ç§’")
    print(f"æ¤œè¨¼ç²¾åº¦: {accuracy:.4f}")
    print(f"æœ€é©åå¾©æ•°: {model.best_iteration_}")

    return model, {
        "algorithm": "LightGBM",
        "accuracy": accuracy,
        "train_time": train_time,
        "n_estimators": model.best_iteration_,
        "learning_rate": 0.05,
    }


def evaluate_model(
    model, X_val: np.ndarray, y_val: np.ndarray, class_names: list[str] = None
) -> dict[str, Any]:
    """è©³ç´°ãªãƒ¢ãƒ‡ãƒ«è©•ä¾¡"""
    y_pred = model.predict(X_val)

    # åŸºæœ¬ãƒ¡ãƒˆãƒªã‚¯ã‚¹
    accuracy = accuracy_score(y_val, y_pred)

    # ã‚¯ãƒ©ã‚¹åˆ¥ãƒ¬ãƒãƒ¼ãƒˆ
    if class_names is None:
        class_names = [f"Class_{i}" for i in range(len(np.unique(y_val)))]

    report = classification_report(
        y_val, y_pred, target_names=class_names, output_dict=True
    )

    # æ··åŒè¡Œåˆ—
    cm = confusion_matrix(y_val, y_pred)

    return {
        "accuracy": accuracy,
        "classification_report": report,
        "confusion_matrix": cm.tolist(),
        "class_names": class_names,
    }


def create_submission(
    model, X_test: np.ndarray, test_df: pd.DataFrame, algorithm_name: str
) -> str:
    """æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"""
    predictions = model.predict(X_test)

    # æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ
    submission_df = pd.DataFrame()

    # IDã‚«ãƒ©ãƒ ã‚’æ¢ã™
    id_col = None
    for col in test_df.columns:
        if "id" in col.lower():
            id_col = col
            break

    if id_col:
        submission_df[id_col] = test_df[id_col]
    else:
        submission_df["id"] = range(len(predictions))

    submission_df["behavior"] = predictions

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"full_dataset_{algorithm_name.lower()}_{timestamp}.csv"
    filepath = Path("submissions") / filename

    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    submission_df.to_csv(filepath, index=False)
    print(f"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {filepath}")

    return str(filepath)


def plot_results(results: list[dict[str, Any]], output_dir: Path):
    """çµæœã‚’å¯è¦–åŒ–"""
    algorithms = [r["algorithm"] for r in results]
    accuracies = [r["accuracy"] for r in results]
    train_times = [r["train_time"] for r in results]

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

    # ç²¾åº¦æ¯”è¼ƒ
    bars1 = ax1.bar(algorithms, accuracies, color=["skyblue", "lightgreen"])
    ax1.set_ylabel("æ¤œè¨¼ç²¾åº¦")
    ax1.set_title("ãƒ¢ãƒ‡ãƒ«ç²¾åº¦æ¯”è¼ƒ")
    ax1.set_ylim(0, 1)

    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, acc in zip(bars1, accuracies, strict=False):
        ax1.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + 0.01,
            f"{acc:.3f}",
            ha="center",
            va="bottom",
        )

    # è¨“ç·´æ™‚é–“æ¯”è¼ƒ
    bars2 = ax2.bar(algorithms, train_times, color=["orange", "lightcoral"])
    ax2.set_ylabel("è¨“ç·´æ™‚é–“ (ç§’)")
    ax2.set_title("è¨“ç·´æ™‚é–“æ¯”è¼ƒ")

    # å€¤ã‚’ãƒãƒ¼ã®ä¸Šã«è¡¨ç¤º
    for bar, time_val in zip(bars2, train_times, strict=False):
        ax2.text(
            bar.get_x() + bar.get_width() / 2,
            bar.get_height() + max(train_times) * 0.01,
            f"{time_val:.1f}s",
            ha="center",
            va="bottom",
        )

    plt.tight_layout()
    plt.savefig(
        output_dir / "full_dataset_comparison.png", dpi=300, bbox_inches="tight"
    )
    plt.show()


def save_results(
    results: list[dict[str, Any]], feature_cols: list[str], output_dir: Path
):
    """çµæœã‚’ä¿å­˜"""
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    summary = {
        "experiment_name": "full_dataset_optimization",
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "total_features": len(feature_cols),
        "feature_names": feature_cols,
        "models": results,
    }

    # JSONå½¢å¼ã§ä¿å­˜
    with open(output_dir / "full_dataset_results.json", "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print(f"çµæœä¿å­˜: {output_dir / 'full_dataset_results.json'}")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå­¦ç¿’å®Ÿé¨“é–‹å§‹")
    print("=" * 50)

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    output_dir = Path("results")
    output_dir.mkdir(exist_ok=True)

    try:
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        train_df, test_df = load_data(use_parquet=True)

        # ç‰¹å¾´é‡æº–å‚™
        X, y, X_test, feature_cols = prepare_features(train_df, test_df)

        # è¨“ç·´ãƒ»æ¤œè¨¼åˆ†å‰²ï¼ˆå±¤åŒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        print("\n=== ãƒ‡ãƒ¼ã‚¿åˆ†å‰² ===")
        X_train, X_val, y_train, y_val = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        print(f"è¨“ç·´ã‚»ãƒƒãƒˆ: {X_train.shape[0]:,} ã‚µãƒ³ãƒ—ãƒ«")
        print(f"æ¤œè¨¼ã‚»ãƒƒãƒˆ: {X_val.shape[0]:,} ã‚µãƒ³ãƒ—ãƒ«")
        print(f"ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆ: {X_test.shape[0]:,} ã‚µãƒ³ãƒ—ãƒ«")

        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’ç¢ºèª
        print("\nè¨“ç·´ã‚»ãƒƒãƒˆã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ:")
        for class_id, count in enumerate(np.bincount(y_train)):
            print(f"  ã‚¯ãƒ©ã‚¹ {class_id}: {count:,} ({count / len(y_train) * 100:.1f}%)")

        # ãƒ¢ãƒ‡ãƒ«è¨“ç·´ã¨è©•ä¾¡
        results = []

        # RandomForest
        rf_model, rf_result = train_random_forest(X_train, y_train, X_val, y_val)
        rf_eval = evaluate_model(rf_model, X_val, y_val)
        rf_result.update(rf_eval)
        results.append(rf_result)

        # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        rf_submission = create_submission(rf_model, X_test, test_df, "RandomForest")
        rf_result["submission_file"] = rf_submission

        # LightGBM
        lgb_model, lgb_result = train_lightgbm(X_train, y_train, X_val, y_val)
        lgb_eval = evaluate_model(lgb_model, X_val, y_val)
        lgb_result.update(lgb_eval)
        results.append(lgb_result)

        # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        lgb_submission = create_submission(lgb_model, X_test, test_df, "LightGBM")
        lgb_result["submission_file"] = lgb_submission

        # çµæœã®å¯è¦–åŒ–ã¨ä¿å­˜
        plot_results(results, output_dir)
        save_results(results, feature_cols, output_dir)

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print("\n" + "=" * 50)
        print("ğŸ‰ å®Ÿé¨“å®Œäº†ã‚µãƒãƒªãƒ¼")
        print("=" * 50)

        best_model = max(results, key=lambda x: x["accuracy"])

        for result in results:
            print(f"\nğŸ“Š {result['algorithm']}:")
            print(f"  æ¤œè¨¼ç²¾åº¦: {result['accuracy']:.4f}")
            print(f"  è¨“ç·´æ™‚é–“: {result['train_time']:.1f}ç§’")
            print(f"  æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: {result['submission_file']}")

        print(
            f"\nğŸ† æœ€é«˜ç²¾åº¦: {best_model['algorithm']} ({best_model['accuracy']:.4f})"
        )

        # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ
        baseline_accuracy = 0.7361  # æ—¢å­˜ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        improvement = best_model["accuracy"] - baseline_accuracy
        print(
            f"ğŸ“ˆ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¯”è¼ƒ: {baseline_accuracy:.4f} â†’ "
            f"{best_model['accuracy']:.4f}"
        )
        print(
            f"   æ”¹å–„: {improvement:+.4f} "
            f"({improvement / baseline_accuracy * 100:+.1f}%)"
        )

        print("\nâœ¨ å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå­¦ç¿’å®Ÿé¨“ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        raise


if __name__ == "__main__":
    main()
