{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMI Competition: Model Training and Evaluation\n",
    "\n",
    "This notebook focuses on training and evaluating machine learning models for behavior detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bfrb.data_processing import SensorDataProcessor\n",
    "from bfrb.evaluation import ModelEvaluator, SubmissionGenerator\n",
    "from bfrb.kaggle_utils import KaggleCompetition\n",
    "from bfrb.models import LightGBMModel, ModelEnsemble, RandomForestModel, XGBoostModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize competition handler\n",
    "competition = KaggleCompetition()\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    train_df = competition.load_train_data()\n",
    "    test_df = competition.load_test_data()\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    print(\"Please run the data exploration notebook first to download the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data processor\n",
    "processor = SensorDataProcessor()\n",
    "\n",
    "# Preprocess training data\n",
    "train_processed = processor.preprocess_sensor_data(train_df, fit=True)\n",
    "train_processed = processor.extract_sensor_features(train_processed)\n",
    "\n",
    "print(f\"Processed training data shape: {train_processed.shape}\")\n",
    "print(f\"Feature columns: {len(processor.feature_columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "# Determine target column\n",
    "target_col = None\n",
    "for col in [\"behavior\", \"target\", \"label\"]:\n",
    "    if col in train_processed.columns:\n",
    "        target_col = col\n",
    "        break\n",
    "\n",
    "if target_col is None:\n",
    "    print(\"Target column not found. Available columns:\")\n",
    "    print(list(train_processed.columns))\n",
    "else:\n",
    "    print(f\"Using target column: {target_col}\")\n",
    "\n",
    "    # Prepare feature matrix and target vector\n",
    "    feature_cols = [col for col in processor.feature_columns if col != target_col]\n",
    "    X = train_processed[feature_cols].fillna(0)  # Fill any remaining NaN values\n",
    "    y = train_processed[target_col]\n",
    "\n",
    "    print(f\"Feature matrix shape: {X.shape}\")\n",
    "    print(f\"Target vector shape: {y.shape}\")\n",
    "    print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Validation set: {X_val.shape}\")\n",
    "    print(f\"Training target distribution: {y_train.value_counts().to_dict()}\")\n",
    "    print(f\"Validation target distribution: {y_val.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Train LightGBM model\n",
    "    lgb_model = LightGBMModel()\n",
    "    print(\"Training LightGBM model...\")\n",
    "    lgb_model.train(X_train.values, y_train.values, X_val.values, y_val.values)\n",
    "    print(\"LightGBM training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Train XGBoost model\n",
    "    xgb_model = XGBoostModel()\n",
    "    print(\"Training XGBoost model...\")\n",
    "    xgb_model.train(X_train.values, y_train.values, X_val.values, y_val.values)\n",
    "    print(\"XGBoost training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestModel(n_estimators=200, max_depth=15)\n",
    "    print(\"Training Random Forest model...\")\n",
    "    rf_model.train(X_train.values, y_train.values)\n",
    "    print(\"Random Forest training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Initialize evaluator\n",
    "    evaluator = ModelEvaluator()\n",
    "\n",
    "    # Collect all models\n",
    "    models = {\"LightGBM\": lgb_model, \"XGBoost\": xgb_model, \"RandomForest\": rf_model}\n",
    "\n",
    "    # Compare models\n",
    "    print(\"Evaluating models on validation set...\")\n",
    "    comparison_results = evaluator.compare_models(models, X_val.values, y_val.values)\n",
    "    print(\"\\n=== MODEL COMPARISON ===\")\n",
    "    display(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Plot feature importance for each model\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n=== {name} Feature Importance ===\")\n",
    "        try:\n",
    "            evaluator.plot_feature_importance(model, feature_cols, top_n=15)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not plot feature importance for {name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Plot confusion matrices\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n=== {name} Confusion Matrix ===\")\n",
    "        y_pred = model.predict(X_val.values)\n",
    "        evaluator.plot_confusion_matrix(y_val.values, y_pred, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Create ensemble model\n",
    "    ensemble = ModelEnsemble(models)\n",
    "\n",
    "    # The models are already trained, so we just need to mark the ensemble as trained\n",
    "    ensemble.is_trained = True\n",
    "\n",
    "    # Evaluate ensemble\n",
    "    print(\"Evaluating ensemble model...\")\n",
    "    ensemble_pred = ensemble.predict(X_val.values)\n",
    "    ensemble_metrics = evaluator.evaluate_model(\n",
    "        ensemble, X_val.values, y_val.values, \"Ensemble\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== ENSEMBLE RESULTS ===\")\n",
    "    for metric, value in ensemble_metrics.items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Plot ensemble confusion matrix\n",
    "    evaluator.plot_confusion_matrix(y_val.values, ensemble_pred, \"Ensemble\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Perform cross-validation on the best model\n",
    "    best_model_name = comparison_results.loc[0, \"model_name\"]\n",
    "    best_model = models[best_model_name]\n",
    "\n",
    "    print(f\"Performing cross-validation on {best_model_name}...\")\n",
    "    cv_results = evaluator.cross_validate_model(\n",
    "        best_model, X.values, y.values, cv_folds=5\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== CROSS-VALIDATION RESULTS ({best_model_name}) ===\")\n",
    "    for metric, value in cv_results.items():\n",
    "        if \"mean\" in metric or \"std\" in metric:\n",
    "            print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Preprocess test data using the same processor\n",
    "    test_processed = processor.preprocess_sensor_data(test_df, fit=False)\n",
    "    test_processed = processor.extract_sensor_features(test_processed)\n",
    "\n",
    "    print(f\"Processed test data shape: {test_processed.shape}\")\n",
    "\n",
    "    # Prepare test features\n",
    "    X_test = test_processed[feature_cols].fillna(0)\n",
    "    print(f\"Test feature matrix shape: {X_test.shape}\")\n",
    "\n",
    "    # Get test IDs\n",
    "    test_ids = test_df[\"id\"] if \"id\" in test_df.columns else range(len(test_df))\n",
    "    print(f\"Test IDs: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    # Initialize submission generator\n",
    "    submission_gen = SubmissionGenerator()\n",
    "\n",
    "    # Generate individual model submissions\n",
    "    submission_files = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nGenerating submission for {name}...\")\n",
    "        submission_file = submission_gen.create_submission(\n",
    "            model, X_test.values, test_ids, f\"{name.lower()}_submission\"\n",
    "        )\n",
    "        submission_files[name] = submission_file\n",
    "\n",
    "    # Generate ensemble submission\n",
    "    print(\"\\nGenerating ensemble submission...\")\n",
    "    ensemble_submission = submission_gen.create_ensemble_submission(\n",
    "        models, X_test.values, test_ids, submission_name=\"ensemble_submission\"\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== SUBMISSION FILES CREATED ===\")\n",
    "    for name, file in submission_files.items():\n",
    "        print(f\"{name}: {file}\")\n",
    "    print(f\"Ensemble: {ensemble_submission}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if target_col is not None:\n",
    "    print(\"=== FINAL MODEL SUMMARY ===\")\n",
    "    print(f\"Best individual model: {comparison_results.loc[0, 'model_name']}\")\n",
    "    print(f\"Best accuracy: {comparison_results.loc[0, 'accuracy']:.4f}\")\n",
    "\n",
    "    print(\"\\n=== RECOMMENDATIONS ===\")\n",
    "    print(\"1. Submit the ensemble model first - it typically performs best\")\n",
    "    print(\"2. Try hyperparameter tuning on the best individual model\")\n",
    "    print(\"3. Consider feature engineering based on feature importance\")\n",
    "    print(\"4. Experiment with different model architectures (neural networks, etc.)\")\n",
    "    print(\"5. Use cross-validation results to assess model stability\")\n",
    "else:\n",
    "    print(\n",
    "        \"No target column found. Please check the data format and update the notebook.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Submit your best model to Kaggle\")\n",
    "print(\"2. Analyze leaderboard feedback\")\n",
    "print(\"3. Iterate on feature engineering and model selection\")\n",
    "print(\"4. Consider advanced techniques like stacking or blending\")\n",
    "print(\"5. Implement time series specific models if applicable\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
