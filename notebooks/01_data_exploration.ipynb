{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMI Competition: Data Exploration\n",
    "\n",
    "This notebook explores the sensor data for body-focused repetitive behavior detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('../src')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from bfrb.kaggle_utils import KaggleCompetition, setup_kaggle_credentials\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Kaggle Credentials and Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle credentials\n",
    "setup_kaggle_credentials()\n",
    "\n",
    "# Initialize competition handler\n",
    "competition = KaggleCompetition()\n",
    "\n",
    "# Download data (this will skip if data already exists)\n",
    "competition.download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "try:\n",
    "    train_df = competition.load_train_data()\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(train_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Training data not found. Please ensure Kaggle credentials are set up correctly.\")\n",
    "    train_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "try:\n",
    "    test_df = competition.load_test_data()\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    print(f\"\\nColumns: {list(test_df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(test_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Test data not found.\")\n",
    "    test_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample submission\n",
    "try:\n",
    "    sample_sub = competition.load_sample_submission()\n",
    "    print(f\"Sample submission shape: {sample_sub.shape}\")\n",
    "    print(f\"\\nColumns: {list(sample_sub.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    display(sample_sub.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Sample submission not found.\")\n",
    "    sample_sub = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Information and Basic Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None:\n",
    "    print(\"=== TRAINING DATA INFO ===\")\n",
    "    print(train_df.info())\n",
    "    print(\"\\n=== DATA TYPES ===\")\n",
    "    print(train_df.dtypes.value_counts())\n",
    "    print(\"\\n=== MISSING VALUES ===\")\n",
    "    missing_values = train_df.isnull().sum()\n",
    "    print(missing_values[missing_values > 0])\n",
    "    print(\"\\n=== BASIC STATISTICS ===\")\n",
    "    display(train_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None and 'behavior' in train_df.columns:\n",
    "    target_col = 'behavior'\n",
    "elif train_df is not None and 'target' in train_df.columns:\n",
    "    target_col = 'target'\n",
    "else:\n",
    "    target_col = None\n",
    "    print(\"Target column not found in the data.\")\n",
    "\n",
    "if target_col and train_df is not None:\n",
    "    print(f\"=== TARGET VARIABLE: {target_col} ===\")\n",
    "    print(f\"Unique values: {train_df[target_col].unique()}\")\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(train_df[target_col].value_counts())\n",
    "    print(\"\\nPercentage distribution:\")\n",
    "    print(train_df[target_col].value_counts(normalize=True) * 100)\n",
    "\n",
    "    # Plot target distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Count plot\n",
    "    train_df[target_col].value_counts().plot(kind='bar', ax=axes[0])\n",
    "    axes[0].set_title('Target Variable Distribution')\n",
    "    axes[0].set_xlabel('Behavior')\n",
    "    axes[0].set_ylabel('Count')\n",
    "\n",
    "    # Pie chart\n",
    "    train_df[target_col].value_counts().plot(kind='pie', ax=axes[1], autopct='%1.1f%%')\n",
    "    axes[1].set_title('Target Variable Percentage')\n",
    "    axes[1].set_ylabel('')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None:\n",
    "    # Identify sensor columns\n",
    "    sensor_keywords = ['accel', 'gyro', 'mag', 'x', 'y', 'z', 'sensor']\n",
    "    sensor_columns = [col for col in train_df.columns if any(keyword in col.lower() for keyword in sensor_keywords)]\n",
    "\n",
    "    print(\"=== SENSOR COLUMNS DETECTED ===\")\n",
    "    print(f\"Total sensor columns: {len(sensor_columns)}\")\n",
    "    for col in sensor_columns[:20]:  # Show first 20\n",
    "        print(f\"  - {col}\")\n",
    "    if len(sensor_columns) > 20:\n",
    "        print(f\"  ... and {len(sensor_columns) - 20} more\")\n",
    "\n",
    "    # Statistical summary of sensor data\n",
    "    if sensor_columns:\n",
    "        print(\"\\n=== SENSOR DATA STATISTICS ===\")\n",
    "        sensor_stats = train_df[sensor_columns].describe()\n",
    "        display(sensor_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis (if timestamp available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None:\n",
    "    # Check for timestamp columns\n",
    "    time_columns = [col for col in train_df.columns if any(keyword in col.lower() for keyword in ['time', 'date', 'timestamp'])]\n",
    "\n",
    "    if time_columns:\n",
    "        print(\"=== TIME COLUMNS DETECTED ===\")\n",
    "        for col in time_columns:\n",
    "            print(f\"  - {col}: {train_df[col].dtype}\")\n",
    "            print(f\"    Sample values: {train_df[col].head(3).tolist()}\")\n",
    "\n",
    "        # Try to convert to datetime\n",
    "        time_col = time_columns[0]\n",
    "        try:\n",
    "            train_df[time_col] = pd.to_datetime(train_df[time_col])\n",
    "            print(f\"\\nSuccessfully converted {time_col} to datetime\")\n",
    "            print(f\"Time range: {train_df[time_col].min()} to {train_df[time_col].max()}\")\n",
    "            print(f\"Duration: {train_df[time_col].max() - train_df[time_col].min()}\")\n",
    "        except:\n",
    "            print(f\"Could not convert {time_col} to datetime\")\n",
    "    else:\n",
    "        print(\"No timestamp columns detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None and len(sensor_columns) > 0:\n",
    "    # Calculate correlations with target if available\n",
    "    if target_col:\n",
    "        correlations = train_df[sensor_columns + [target_col]].corr()[target_col].abs().sort_values(ascending=False)\n",
    "        print(\"=== TOP CORRELATIONS WITH TARGET ===\")\n",
    "        print(correlations.head(20))\n",
    "\n",
    "        # Plot top correlations\n",
    "        top_corr = correlations.head(15)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        top_corr[1:].plot(kind='bar')  # Exclude target itself\n",
    "        plt.title('Top 15 Feature Correlations with Target')\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Absolute Correlation')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Correlation heatmap for a subset of features\n",
    "    if len(sensor_columns) > 5:\n",
    "        sample_features = sensor_columns[:10]  # Take first 10 for visualization\n",
    "        if target_col:\n",
    "            sample_features.append(target_col)\n",
    "\n",
    "        corr_matrix = train_df[sample_features].corr()\n",
    "\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                   square=True, fmt='.2f')\n",
    "        plt.title('Correlation Heatmap (Sample Features)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None and len(sensor_columns) > 0:\n",
    "    # Plot sample sensor data\n",
    "    sample_size = min(1000, len(train_df))\n",
    "    sample_data = train_df.sample(n=sample_size, random_state=42)\n",
    "\n",
    "    # Select a few sensor columns for visualization\n",
    "    viz_columns = sensor_columns[:4]  # First 4 sensor columns\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(viz_columns):\n",
    "        if target_col:\n",
    "            # Separate by target class\n",
    "            for target_val in sample_data[target_col].unique():\n",
    "                subset = sample_data[sample_data[target_col] == target_val]\n",
    "                axes[i].scatter(range(len(subset)), subset[col],\n",
    "                              alpha=0.6, label=f'Target {target_val}', s=10)\n",
    "        else:\n",
    "            axes[i].plot(sample_data[col], alpha=0.7)\n",
    "\n",
    "        axes[i].set_title(f'{col}')\n",
    "        axes[i].set_xlabel('Sample Index')\n",
    "        axes[i].set_ylabel('Value')\n",
    "        if target_col:\n",
    "            axes[i].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_df is not None:\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicates = train_df.duplicated().sum()\n",
    "    print(f\"Duplicate rows: {duplicates}\")\n",
    "\n",
    "    # Check for constant columns\n",
    "    constant_cols = []\n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].nunique() <= 1:\n",
    "            constant_cols.append(col)\n",
    "\n",
    "    print(f\"Constant columns: {len(constant_cols)}\")\n",
    "    if constant_cols:\n",
    "        print(f\"  - {constant_cols}\")\n",
    "\n",
    "    # Check for high cardinality columns\n",
    "    high_card_cols = []\n",
    "    for col in train_df.columns:\n",
    "        if train_df[col].dtype == 'object' and train_df[col].nunique() > len(train_df) * 0.8:\n",
    "            high_card_cols.append(col)\n",
    "\n",
    "    print(f\"High cardinality columns: {len(high_card_cols)}\")\n",
    "    if high_card_cols:\n",
    "        print(f\"  - {high_card_cols}\")\n",
    "\n",
    "    # Memory usage\n",
    "    memory_usage = train_df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"\\nMemory usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "    # Data types summary\n",
    "    print(\"\\nData types summary:\")\n",
    "    print(train_df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA EXPLORATION SUMMARY ===\")\n",
    "if train_df is not None:\n",
    "    print(f\"✓ Training data loaded: {train_df.shape}\")\n",
    "    print(f\"✓ Sensor columns identified: {len(sensor_columns)}\")\n",
    "    if target_col:\n",
    "        print(f\"✓ Target variable: {target_col}\")\n",
    "        print(f\"✓ Class distribution: {dict(train_df[target_col].value_counts())}\")\n",
    "    print(f\"✓ Missing values: {train_df.isnull().sum().sum()}\")\n",
    "    print(f\"✓ Duplicate rows: {train_df.duplicated().sum()}\")\n",
    "else:\n",
    "    print(\"✗ Training data not loaded - check Kaggle credentials\")\n",
    "\n",
    "if test_df is not None:\n",
    "    print(f\"✓ Test data loaded: {test_df.shape}\")\n",
    "else:\n",
    "    print(\"✗ Test data not loaded\")\n",
    "\n",
    "print(\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Data preprocessing and feature engineering\")\n",
    "print(\"2. Model development and training\")\n",
    "print(\"3. Model evaluation and selection\")\n",
    "print(\"4. Submission generation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
